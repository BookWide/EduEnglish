# worker.py (Final - Supabase Storage version)
# - Claim queued jobs via RPC: public.claim_next_media_job(_worker_id text, _lease_seconds int) -> media_jobs row
# - Generate images per beat, upload to Storage
# - Optional: TTS narration, BGM mix, subtitles burn-in, MP4 compose, ZIP pack
# - Progress + retry + pause/resume control via status: pending/queued/processing/done/failed
#
# Env required:
#   SUPABASE_URL
#   SUPABASE_SERVICE_ROLE
#   OPENAI_API_KEY
#
# Optional:
#   SUPABASE_BUCKET (default: talk)
#   WORKER_ID (default: WIN-<hostname>)
#   LEASE_SECONDS (default: 300)
#   POLL_SECONDS (default: 3)
#   OPENAI_MODEL_ANALYZE (default: gpt-4o-mini)
#   OPENAI_MODEL_IMAGE (default: gpt-image-1)
#   OPENAI_MODEL_TTS (default: gpt-4o-mini-tts)
#   ENABLE_TTS (default: 1)
#   ENABLE_SUBTITLES (default: 1)
#   ENABLE_BGM (default: 0)
#   BGM_FILE (path to local mp3/wav)
#   FFMPEG_PATH (default: ffmpeg)
#   FONT_FILE (path to a .ttf, for drawtext fallback; subtitles filter doesn't require it)
#
# Tables expected (minimum):
#   public.media_jobs: id(uuid), status(text), stage(text), progress(int), error_code(text), error_message(text),
#                     worker_id(text), lease_until(timestamptz),
#                     slug(text), category(text), title(text),
#                     output_zip_url(text), output_manifest_url(text)
#   public.media_job_beats: id(uuid), job_id(uuid), beat_index(int), start_ms(int), end_ms(int),
#                           line_text(text), scene_index(int null), photo_prompt(text null)
#
# Storage layout:
#   <bucket>/jobs/<job_id>/1.png ... N.png
#   <bucket>/jobs/<job_id>/narration.mp3 (optional)
#   <bucket>/jobs/<job_id>/subtitles.srt
#   <bucket>/jobs/<job_id>/output.mp4
#   <bucket>/jobs/<job_id>/output.zip
#   <bucket>/jobs/<job_id>/manifest.json

import os
import re
import io
import json
import time
import math
import socket
import shutil
import zipfile
import tempfile
import subprocess
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import httpx

# supabase-py
from supabase import create_client, Client

# openai (new style)
from openai import OpenAI


# ---------------------------
# Config
# ---------------------------
SUPABASE_URL = os.environ.get("SUPABASE_URL", "").strip()
SUPABASE_SERVICE_ROLE = os.environ.get("SUPABASE_SERVICE_ROLE", "").strip()
SUPABASE_BUCKET = os.environ.get("SUPABASE_BUCKET", "talk").strip()

WORKER_ID = os.environ.get("WORKER_ID", f"WIN-{socket.gethostname()}").strip()
LEASE_SECONDS = int(os.environ.get("LEASE_SECONDS", "300"))
POLL_SECONDS = int(os.environ.get("POLL_SECONDS", "3"))

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
OPENAI_MODEL_ANALYZE = os.environ.get("OPENAI_MODEL_ANALYZE", "gpt-4o-mini").strip()
OPENAI_MODEL_IMAGE = os.environ.get("OPENAI_MODEL_IMAGE", "gpt-image-1").strip()
OPENAI_MODEL_TTS = os.environ.get("OPENAI_MODEL_TTS", "gpt-4o-mini-tts").strip()

ENABLE_TTS = os.environ.get("ENABLE_TTS", "1").strip() == "1"
ENABLE_SUBTITLES = os.environ.get("ENABLE_SUBTITLES", "1").strip() == "1"
ENABLE_BGM = os.environ.get("ENABLE_BGM", "0").strip() == "1"

BGM_FILE = os.environ.get("BGM_FILE", "").strip()
FFMPEG_PATH = os.environ.get("FFMPEG_PATH", "ffmpeg").strip()

# If you want to use drawtext (fallback) you can provide FONT_FILE
FONT_FILE = os.environ.get("FONT_FILE", "").strip()

# Confidence thresholds
MIN_ERA_CONF = 0.65
MIN_GENDER_CONF = 0.60
MIN_PROTAG_CONF = 0.60

# Retry
RETRY_MAX = 4
RETRY_BASE_SLEEP = 1.2  # seconds

# ---------------------------
# Helpers
# ---------------------------

def die(msg: str) -> None:
    raise RuntimeError(msg)

def now_iso() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def safe_job_folder(job_id: str) -> str:
    return f"jobs/{job_id}"

def storage_public_url(supabase_url: str, bucket: str, path: str) -> str:
    # Public buckets: /storage/v1/object/public/<bucket>/<path>
    return f"{supabase_url}/storage/v1/object/public/{bucket}/{path}"

def ms_to_srt_ts(ms: int) -> str:
    if ms < 0:
        ms = 0
    h = ms // 3600000
    ms -= h * 3600000
    m = ms // 60000
    ms -= m * 60000
    s = ms // 1000
    ms -= s * 1000
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

def backoff_sleep(attempt: int) -> None:
    # attempt starts from 1
    t = RETRY_BASE_SLEEP * (2 ** (attempt - 1))
    t = min(t, 12.0)
    time.sleep(t)

def is_transient_error(e: Exception) -> bool:
    msg = str(e).lower()
    transient_keywords = [
        "timeout", "timed out", "server disconnected", "temporarily",
        "rate limit", "429", "502", "503", "504", "connection reset",
        "getaddrinfo failed", "remoteprotocolerror", "network", "dns"
    ]
    return any(k in msg for k in transient_keywords)


@dataclass
class Beat:
    beat_index: int
    start_ms: int
    end_ms: int
    line_text: str
    photo_prompt: Optional[str] = None


# ---------------------------
# Supabase / OpenAI clients
# ---------------------------
def build_supabase() -> Client:
    if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE:
        die("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE env.")
    return create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE)

def build_openai() -> OpenAI:
    if not OPENAI_API_KEY:
        die("Missing OPENAI_API_KEY env.")
    return OpenAI(api_key=OPENAI_API_KEY)


# ---------------------------
# DB ops
# ---------------------------
def update_job(supabase: Client, job_id: str, **fields: Any) -> None:
    fields["updated_at"] = "now()"  # let DB handle if you have trigger; harmless if ignored
    # supabase-py doesn't accept SQL functions in update directly; so we won't set updated_at.
    fields.pop("updated_at", None)
    supabase.table("media_jobs").update(fields).eq("id", job_id).execute()

def mark_failed(supabase: Client, job_id: str, code: str, message: str) -> None:
    update_job(
        supabase,
        job_id,
        status="failed",
        stage="failed",
        progress=0,
        error_code=code[:80],
        error_message=message[:500],
        worker_id=WORKER_ID,
    )

def load_beats(supabase: Client, job_id: str) -> List[Beat]:
    # order by beat_index
    res = supabase.table("media_job_beats").select("beat_index,start_ms,end_ms,line_text,photo_prompt").eq("job_id", job_id).order("beat_index").execute()
    rows = res.data or []
    beats: List[Beat] = []
    for r in rows:
        beats.append(
            Beat(
                beat_index=int(r["beat_index"]),
                start_ms=int(r.get("start_ms") or 0),
                end_ms=int(r.get("end_ms") or 0),
                line_text=str(r.get("line_text") or ""),
                photo_prompt=r.get("photo_prompt"),
            )
        )
    return beats

def claim_job(supabase: Client) -> Optional[Dict[str, Any]]:
    # Only claim jobs whose status='queued' (handled in RPC)
    try:
        payload = {"_worker_id": WORKER_ID, "_lease_seconds": LEASE_SECONDS}
        res = supabase.rpc("claim_next_media_job", payload).execute()
        # Supabase RPC returns single row or None
        if not res.data:
            return None
        # Some supabase versions return dict, others list[dict]
        if isinstance(res.data, list):
            return res.data[0] if res.data else None
        return res.data
    except Exception as e:
        # Don't crash worker loop
        print(f"claim_job exception: {e}")
        return None


# ---------------------------
# Storage upload
# ---------------------------
def storage_remove_if_exists(supabase: Client, bucket: str, path: str) -> None:
    try:
        supabase.storage.from_(bucket).remove([path])
    except Exception:
        pass

def storage_upload_bytes(supabase: Client, bucket: str, path: str, content: bytes, content_type: str) -> str:
    # Important: file_options values must be strings (avoid bool -> header error)
    file_options = {"content-type": str(content_type), "cache-control": "3600"}
    # overwrite: remove first (supabase storage upload doesn't always accept upsert)
    storage_remove_if_exists(supabase, bucket, path)
    supabase.storage.from_(bucket).upload(path, content, file_options=file_options)
    return storage_public_url(SUPABASE_URL, bucket, path)

def storage_upload_file(supabase: Client, bucket: str, path: str, file_path: str, content_type: str) -> str:
    with open(file_path, "rb") as f:
        data = f.read()
    return storage_upload_bytes(supabase, bucket, path, data, content_type)


# ---------------------------
# AI Analysis (mode 3)
# ---------------------------
ANALYZE_SYSTEM = """
You are a film/story analyst. Extract only what is supported by the given subtitles.
Return STRICT JSON only. No markdown. No extra text.
If uncertain, set field to null and set low confidence.

Schema:
{
  "era": {"label": string|null, "confidence": number, "evidence": [string]},
  "protagonists": [{"name": string, "gender": "male"|"female"|"unknown", "confidence": number, "evidence": [string]}],
  "visual_style": string,
  "fallback": "narration_only"|"speaker_dialogue"
}
"""

def extract_json(text: str) -> Optional[dict]:
    text = (text or "").strip()
    if not text:
        return None
    # try direct
    try:
        return json.loads(text)
    except Exception:
        pass
    # try find first {...}
    m = re.search(r"\{.*\}", text, flags=re.S)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None

def analyze_script(openai_client: OpenAI, beats: List[Beat]) -> Dict[str, Any]:
    # Use first N lines + some summary
    N = min(30, len(beats))
    sample = "\n".join([f"{b.beat_index}. {clean_text(b.line_text)}" for b in beats[:N]])
    full_stats = f"Total lines: {len(beats)}. First line: {clean_text(beats[0].line_text) if beats else ''}"
    user = f"Subtitles sample:\n{sample}\n\nStats:\n{full_stats}\n\nReturn JSON schema."

    resp = openai_client.chat.completions.create(
        model=OPENAI_MODEL_ANALYZE,
        messages=[
            {"role": "system", "content": ANALYZE_SYSTEM.strip()},
            {"role": "user", "content": user},
        ],
        temperature=0.2,
    )
    content = resp.choices[0].message.content if resp and resp.choices else ""
    data = extract_json(content) or {}
    # normalize
    era = data.get("era") or {}
    if "confidence" not in era:
        era["confidence"] = 0.0
    if "evidence" not in era:
        era["evidence"] = []
    prot = data.get("protagonists") or []
    style = data.get("visual_style") or "cinematic, storybook, soft film lighting"
    fallback = data.get("fallback") or "narration_only"
    return {"era": era, "protagonists": prot, "visual_style": style, "fallback": fallback}


def choose_cast(analysis: Dict[str, Any]) -> Dict[str, Any]:
    """
    Decide final parameters with confidence thresholds.
    """
    era = analysis.get("era") or {}
    era_label = era.get("label") if float(era.get("confidence", 0.0)) >= MIN_ERA_CONF else None

    protagonists = analysis.get("protagonists") or []
    main = None
    if protagonists:
        # pick highest confidence
        protagonists = sorted(protagonists, key=lambda x: float(x.get("confidence", 0.0)), reverse=True)
        if float(protagonists[0].get("confidence", 0.0)) >= MIN_PROTAG_CONF:
            main = protagonists[0]

    gender = "unknown"
    if main:
        g = main.get("gender") or "unknown"
        gc = float(main.get("confidence", 0.0))
        # only accept gender if confidence high enough AND provided
        if g in ("male", "female") and gc >= MIN_GENDER_CONF:
            gender = g

    style = analysis.get("visual_style") or "cinematic, storybook, soft film lighting"
    return {
        "era_label": era_label,
        "main_character": main.get("name") if main else None,
        "main_gender": gender,
        "visual_style": style,
    }


# ---------------------------
# Image generation
# ---------------------------
def build_prompt(cast: Dict[str, Any], beat: Beat) -> str:
    # If beat has photo_prompt, prefer it
    base = beat.photo_prompt.strip() if beat.photo_prompt else ""
    line = clean_text(beat.line_text)

    era = cast.get("era_label")
    main = cast.get("main_character")
    gender = cast.get("main_gender")
    style = cast.get("visual_style")

    hints = []
    if era:
        hints.append(f"Era: {era}")
    if main:
        hints.append(f"Main character: {main} ({gender})")
    hints.append(f"Style: {style}")
    hint_text = "; ".join(hints)

    if base:
        return f"{base}\n\nSubtitles line: {line}\n\n{hint_text}"
    return f"Scene inspired by: {line}\n\n{hint_text}\nCinematic composition, clean frame, no text, no subtitles."

def gen_image_png(openai_client: OpenAI, prompt: str) -> bytes:
    # OpenAI Images API (gpt-image-1) returns base64 in response
    img = openai_client.images.generate(
        model=OPENAI_MODEL_IMAGE,
        prompt=prompt,
        size="1024x1024",
    )
    b64 = img.data[0].b64_json
    import base64
    return base64.b64decode(b64)


# ---------------------------
# TTS
# ---------------------------
def gen_tts_mp3(openai_client: OpenAI, text: str) -> bytes:
    # Keep it short per line; caller will concatenate
    # Use a stable voice name; adjust if you already have preferred voices.
    # If your model doesn't support mp3, switch format in code below.
    speech = openai_client.audio.speech.create(
        model=OPENAI_MODEL_TTS,
        voice="alloy",
        input=text,
        format="mp3",
    )
    return speech.read()


def concat_mp3(files: List[str], out_path: str) -> None:
    # Use ffmpeg concat demuxer for mp3
    with tempfile.NamedTemporaryFile("w", suffix=".txt", delete=False, encoding="utf-8") as f:
        for p in files:
            f.write(f"file '{p.replace("'", r"\'")}'\n")
        list_path = f.name
    try:
        cmd = [FFMPEG_PATH, "-y", "-f", "concat", "-safe", "0", "-i", list_path, "-c", "copy", out_path]
        run_cmd(cmd)
    finally:
        try:
            os.remove(list_path)
        except Exception:
            pass


# ---------------------------
# Subtitle file
# ---------------------------
def build_srt(beats: List[Beat], out_path: str) -> None:
    lines = []
    for i, b in enumerate(beats, start=1):
        start = ms_to_srt_ts(b.start_ms)
        end = ms_to_srt_ts(b.end_ms)
        txt = clean_text(b.line_text)
        lines.append(str(i))
        lines.append(f"{start} --> {end}")
        lines.append(txt)
        lines.append("")  # blank line
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


# ---------------------------
# FFmpeg compose
# ---------------------------
def run_cmd(cmd: List[str]) -> None:
    # Raises on failure
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding="utf-8", errors="ignore")
    if p.returncode != 0:
        raise RuntimeError(p.stdout[-2000:])

def make_slideshow_video(image_paths: List[str], beats: List[Beat], out_mp4: str, srt_path: Optional[str], narration_mp3: Optional[str]) -> None:
    """
    Build a simple slideshow MP4:
    - each image duration derived from beats timing
    - 25fps, 1024x1024 (images already 1024)
    - if narration_mp3 exists: add audio
    - if srt_path exists: burn subtitles
    """
    # Build concat list with durations
    # Note: concat demuxer expects duration lines; last file's duration ignored unless repeated
    with tempfile.NamedTemporaryFile("w", suffix=".txt", delete=False, encoding="utf-8") as f:
        for idx, img in enumerate(image_paths):
            dur = max(0.5, (beats[idx].end_ms - beats[idx].start_ms) / 1000.0)
            f.write(f"file '{img.replace("'", r"\'")}'\n")
            f.write(f"duration {dur:.3f}\n")
        # repeat last file to make duration apply
        f.write(f"file '{image_paths[-1].replace("'", r"\'")}'\n")
        list_path = f.name

    vf_filters = []
    # scale/pad safety
    vf_filters.append("scale=1024:1024:force_original_aspect_ratio=decrease,pad=1024:1024:(ow-iw)/2:(oh-ih)/2")

    if srt_path and ENABLE_SUBTITLES:
        # Burn subtitles (requires libass on ffmpeg build; most windows builds have it)
        # Escape path for ffmpeg filter
        sp = srt_path.replace("\\", "\\\\").replace(":", "\\:")
        vf_filters.append(f"subtitles='{sp}'")

    vf = ",".join(vf_filters)

    try:
        cmd = [FFMPEG_PATH, "-y",
               "-f", "concat", "-safe", "0", "-i", list_path,
               "-r", "25",
               "-vf", vf,
               "-c:v", "libx264", "-pix_fmt", "yuv420p",
               "-movflags", "+faststart"]
        if narration_mp3:
            cmd += ["-i", narration_mp3, "-c:a", "aac", "-shortest"]
        cmd += [out_mp4]
        run_cmd(cmd)
    finally:
        try:
            os.remove(list_path)
        except Exception:
            pass


def mix_bgm(narration_mp3: str, bgm_file: str, out_mp3: str) -> None:
    # narration at 1.0, bgm at 0.18 (ducked)
    cmd = [
        FFMPEG_PATH, "-y",
        "-i", narration_mp3,
        "-i", bgm_file,
        "-filter_complex", "[1:a]volume=0.18[a1];[0:a][a1]amix=inputs=2:duration=first:dropout_transition=2[aout]",
        "-map", "[aout]",
        "-c:a", "mp3",
        out_mp3
    ]
    run_cmd(cmd)


# ---------------------------
# ZIP + Manifest
# ---------------------------
def make_manifest(job: Dict[str, Any], beats: List[Beat], cast: Dict[str, Any], urls: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "job_id": job.get("id"),
        "slug": job.get("slug"),
        "category": job.get("category"),
        "title": job.get("title"),
        "generated_at": now_iso(),
        "cast": cast,
        "outputs": urls,
        "beats": [
            {
                "beat_index": b.beat_index,
                "start_ms": b.start_ms,
                "end_ms": b.end_ms,
                "line_text": b.line_text,
                "photo_prompt": b.photo_prompt,
                "image_url": urls.get("images", {}).get(str(b.beat_index)),
            }
            for b in beats
        ],
    }


def zip_folder(folder: str, out_zip: str) -> None:
    with zipfile.ZipFile(out_zip, "w", zipfile.ZIP_DEFLATED) as z:
        for root, _, files in os.walk(folder):
            for fn in files:
                p = os.path.join(root, fn)
                rel = os.path.relpath(p, folder)
                z.write(p, rel)


# ---------------------------
# Main job processing
# ---------------------------
def process_job(supabase: Client, openai_client: OpenAI, job: Dict[str, Any]) -> None:
    job_id = job["id"]
    print(f"\n=== Generating job: {job_id} ===")

    # Pause control: worker only processes queued (claimed by RPC). If someone sets pending, it won't be claimed.
    update_job(supabase, job_id, status="processing", stage="start", progress=1, worker_id=WORKER_ID, error_code=None, error_message=None)

    beats = load_beats(supabase, job_id)
    if not beats:
        mark_failed(supabase, job_id, "NO_BEATS", "No beats found for this job_id.")
        return

    # --- AI analyze & choose cast (Mode 3)
    update_job(supabase, job_id, stage="analyze", progress=3)
    analysis = {}
    for attempt in range(1, RETRY_MAX + 1):
        try:
            analysis = analyze_script(openai_client, beats)
            break
        except Exception as e:
            print(f"Analyze attempt {attempt} failed: {e}")
            if attempt == RETRY_MAX or not is_transient_error(e):
                # fallback safe
                analysis = {"era": {"label": None, "confidence": 0.0, "evidence": []}, "protagonists": [], "visual_style": "cinematic, storybook, soft film lighting", "fallback": "narration_only"}
                break
            backoff_sleep(attempt)

    cast = choose_cast(analysis)

    # --- Temp workspace
    workdir = tempfile.mkdtemp(prefix=f"job_{job_id}_")
    try:
        job_folder = os.path.join(workdir, "job")
        os.makedirs(job_folder, exist_ok=True)

        # --- Generate images
        update_job(supabase, job_id, stage="images", progress=5)
        image_local_paths: List[str] = []
        image_urls: Dict[str, str] = {}
        total = len(beats)

        for idx, b in enumerate(beats, start=1):
            pct = 5 + int(55 * (idx / total))  # 5..60
            update_job(supabase, job_id, stage=f"image_{b.beat_index}", progress=pct)

            prompt = build_prompt(cast, b)
            print(f"Beat {b.beat_index}/{total} prompt: {prompt[:90]}...")

            # retry image generation
            png_bytes = None
            for attempt in range(1, RETRY_MAX + 1):
                try:
                    png_bytes = gen_image_png(openai_client, prompt)
                    break
                except Exception as e:
                    print(f"Image gen attempt {attempt} failed: {e}")
                    if attempt == RETRY_MAX or not is_transient_error(e):
                        raise
                    backoff_sleep(attempt)

            local_png = os.path.join(job_folder, f"{b.beat_index}.png")
            with open(local_png, "wb") as f:
                f.write(png_bytes)

            # upload
            remote_path = f"{safe_job_folder(job_id)}/{b.beat_index}.png"
            url = None
            for attempt in range(1, RETRY_MAX + 1):
                try:
                    url = storage_upload_bytes(supabase, SUPABASE_BUCKET, remote_path, png_bytes, "image/png")
                    break
                except Exception as e:
                    print(f"Upload attempt {attempt} failed: {e}")
                    if attempt == RETRY_MAX or not is_transient_error(e):
                        raise
                    backoff_sleep(attempt)

            image_local_paths.append(local_png)
            image_urls[str(b.beat_index)] = url

        # --- Build subtitles.srt (local + upload)
        srt_path = os.path.join(job_folder, "subtitles.srt")
        build_srt(beats, srt_path)
        srt_url = storage_upload_file(supabase, SUPABASE_BUCKET, f"{safe_job_folder(job_id)}/subtitles.srt", srt_path, "application/x-subrip")

        # --- TTS narration (optional)
        narration_mp3 = None
        narration_url = None
        if ENABLE_TTS:
            update_job(supabase, job_id, stage="tts", progress=62)
            tts_parts: List[str] = []
            for idx, b in enumerate(beats, start=1):
                # Keep each chunk short
                text = clean_text(b.line_text)
                if not text:
                    continue
                chunk = os.path.join(job_folder, f"tts_{idx}.mp3")

                mp3_bytes = None
                for attempt in range(1, RETRY_MAX + 1):
                    try:
                        mp3_bytes = gen_tts_mp3(openai_client, text)
                        break
                    except Exception as e:
                        print(f"TTS attempt {attempt} failed: {e}")
                        if attempt == RETRY_MAX or not is_transient_error(e):
                            raise
                        backoff_sleep(attempt)

                with open(chunk, "wb") as f:
                    f.write(mp3_bytes)
                tts_parts.append(chunk)

            if tts_parts:
                narration_mp3 = os.path.join(job_folder, "narration.mp3")
                concat_mp3(tts_parts, narration_mp3)

                # optional BGM mix
                if ENABLE_BGM and BGM_FILE and os.path.exists(BGM_FILE):
                    update_job(supabase, job_id, stage="bgm_mix", progress=68)
                    mixed = os.path.join(job_folder, "narration_bgm.mp3")
                    mix_bgm(narration_mp3, BGM_FILE, mixed)
                    narration_mp3 = mixed

                narration_url = storage_upload_file(supabase, SUPABASE_BUCKET, f"{safe_job_folder(job_id)}/narration.mp3", narration_mp3, "audio/mpeg")

        # --- Compose MP4
        update_job(supabase, job_id, stage="mp4", progress=75)
        out_mp4 = os.path.join(job_folder, "output.mp4")

        # retry ffmpeg
        for attempt in range(1, RETRY_MAX + 1):
            try:
                make_slideshow_video(
                    image_paths=image_local_paths,
                    beats=beats,
                    out_mp4=out_mp4,
                    srt_path=srt_path if ENABLE_SUBTITLES else None,
                    narration_mp3=narration_mp3 if ENABLE_TTS else None,
                )
                break
            except Exception as e:
                print(f"FFmpeg attempt {attempt} failed: {e}")
                if attempt == RETRY_MAX or not is_transient_error(e):
                    raise
                backoff_sleep(attempt)

        mp4_url = storage_upload_file(supabase, SUPABASE_BUCKET, f"{safe_job_folder(job_id)}/output.mp4", out_mp4, "video/mp4")

        # --- manifest.json
        update_job(supabase, job_id, stage="manifest", progress=85)
        outputs = {
            "mp4_url": mp4_url,
            "srt_url": srt_url,
            "narration_url": narration_url,
            "images": image_urls,
        }
        manifest = make_manifest(job, beats, cast, outputs)
        manifest_path = os.path.join(job_folder, "manifest.json")
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        manifest_url = storage_upload_file(
            supabase,
            SUPABASE_BUCKET,
            f"{safe_job_folder(job_id)}/manifest.json",
            manifest_path,
            "application/json",
        )

        # --- ZIP pack (pack local job folder)
        update_job(supabase, job_id, stage="zip", progress=92)
        out_zip = os.path.join(workdir, "output.zip")
        zip_folder(job_folder, out_zip)

        zip_url = storage_upload_file(
            supabase,
            SUPABASE_BUCKET,
            f"{safe_job_folder(job_id)}/output.zip",
            out_zip,
            "application/zip",
        )

        # --- Write outputs to DB (only columns that exist)
        update_job(
            supabase,
            job_id,
            status="done",
            stage="done",
            progress=100,
            output_zip_url=zip_url,
            output_manifest_url=manifest_url,
            error_code=None,
            error_message=None,
        )

        print(f"Job done: {job_id}")
        print(f"MP4: {mp4_url}")
        print(f"ZIP: {zip_url}")
        print(f"Manifest: {manifest_url}")

    except Exception as e:
        print(f"Worker error: {e}")
        try:
            mark_failed(supabase, job_id, "WORKER_ERROR", str(e))
        except Exception as ee:
            print(f"Failed to mark job failed: {ee}")
    finally:
        try:
            shutil.rmtree(workdir, ignore_errors=True)
        except Exception:
            pass


# ---------------------------
# Self test
# ---------------------------
def startup_self_test(supabase: Client) -> None:
    try:
        test = supabase.table("media_job_beats").select("id,beat_index,line_text").limit(1).execute()
        rows = test.data or []
        print("TEST beats len =", len(rows))
        if rows:
            print("TEST first beat =", rows[0])
        else:
            print("TEST beats is empty (DB ok, just no beats)")
    except Exception as e:
        print(f"TEST query exception: {type(e).__name__}: {e}")


def main() -> None:
    print("Worker started.")
    print("SUPABASE_URL =", SUPABASE_URL)
    print("SUPABASE_BUCKET =", SUPABASE_BUCKET)
    print("WORKER_ID =", WORKER_ID)
    print("LEASE_SECONDS =", LEASE_SECONDS)
    print("FFMPEG_PATH =", FFMPEG_PATH)
    print("ENABLE_TTS =", ENABLE_TTS, "ENABLE_SUBTITLES =", ENABLE_SUBTITLES, "ENABLE_BGM =", ENABLE_BGM)

    supabase = build_supabase()
    openai_client = build_openai()

    startup_self_test(supabase)

    try:
        while True:
            job = claim_job(supabase)
            if job:
                # IMPORTANT: If someone set it back to pending while we are processing,
                # your RPC should prevent claiming; but if it happens mid-run, we still finish current run.
                process_job(supabase, openai_client, job)
            else:
                print("No job. Sleeping...")
                time.sleep(POLL_SECONDS)
    except KeyboardInterrupt:
        print("\nWorker stopped (Ctrl+C).")


if __name__ == "__main__":
    main()



